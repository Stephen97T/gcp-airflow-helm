# ============================================
# Airflow Helm Chart Configuration - Local testing version
# Optimized for: Minikube (local)
# ============================================

# 1. EXECUTOR CONFIGURATION
# KubernetesExecutor = Each task runs in its own pod
executor: "KubernetesExecutor"

# 2. AIRFLOW IMAGE
images:
  airflow:
    repository: apache/airflow
    tag: "3.1.0"  # Upgrade to this version for testing
    pullPolicy: IfNotPresent

# 3. DATABASE (In-Cluster PostgreSQL)
postgresql:
  enabled: true
  image:
    repository: bitnami/postgresql
    tag: "latest"
  auth:
    username: airflow
    password: airflow  # Use a strong password in production!
    database: airflow
  persistence:
    enabled: true
    size: 5Gi
    storageClass: standard  # Works with Minikube

# 4. DAG LOADING VIA GIT-SYNC
# DAGs are pulled from GitHub (no need to rebuild images)
# can disable for initial deployment if no dags yet, then enable after creating DAGs
dags:
  gitSync:
    enabled: true
    repo: https://github.com/Stephen97T/gcp-airflow-helm.git # Update with your repo URL
    branch: main
    subPath: "dags"  # Folder containing DAG files
    wait: 60  # Check for updates every 60 seconds
    maxFailures: 3

# 5. WEBSERVER CONFIGURATION
webserver:
  replicas: 1
  service:
    type: LoadBalancer  # Use NodePort for Minikube
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"
  defaultUser:
    enabled: true
    role: Admin
    username: admin
    email: admin@example.com
    firstName: Admin
    lastName: Admin
    password: admin # Use a strong password in production!


# 6. SCHEDULER CONFIGURATION
scheduler:
  replicas: 1
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

# internal API new component airflow 3
internalApi:
  enabled: true
  replicas: 1  # for production consider setting higher for higher availability
  resources:
    limits:
      cpu: 500m
      memory: 512Mi

# 7. TRIGGERER (Airflow 2.x Feature for Deferrable Operators)
triggerer:
  enabled: true
  replicas: 1
  resources:
    requests:
      cpu: "250m"
      memory: "512Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"

# 8. WORKERS (Not used with KubernetesExecutor, but keeping config)
workers:
  replicas: 0  # KubernetesExecutor spawns pods on-demand
  resources:
    requests:
      cpu: "250m"
      memory: "512Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"

# 9. REDIS (Not needed for KubernetesExecutor)
redis:
  enabled: false

# 10. ENVIRONMENT VARIABLES
env:
  - name: RUN_TARGET_NAMESPACE
    value: "development"
  - name: AIRFLOW__CORE__LOAD_EXAMPLES
    value: "False"  # Don't load example DAGs
  - name: AIRFLOW__WEBSERVER__EXPOSE_CONFIG
    value: "True"  # Useful for debugging
  - name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
    value: "True"  # New DAGs start paused

# 11. LOGGING
logs:
  persistence:
    enabled: true
    size: 5Gi
    storageClassName: standard

# 13. EXTRA CONFIGURATION
# Disable StatefulSet pod management policy for faster updates
statefulset:
  podManagementPolicy: Parallel

# 14. FLOWER (Optional Web UI for Celery - not needed for KubernetesExecutor)
flower:
  enabled: false
